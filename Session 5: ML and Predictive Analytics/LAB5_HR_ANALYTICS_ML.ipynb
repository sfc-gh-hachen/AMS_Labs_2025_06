{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "import_packages"
   },
   "outputs": [],
   "source": [
    "# Import required libraries for Snowflake ML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from snowflake.snowpark import Window\n",
    "import snowflake.snowpark.functions as F\n",
    "from snowflake.snowpark.types import LongType\n",
    "from snowflake.ml.modeling.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from snowflake.ml.modeling.xgboost import XGBClassifier\n",
    "from snowflake.ml.modeling.model_selection import GridSearchCV\n",
    "from snowflake.ml.modeling.metrics import accuracy_score, roc_auc_score\n",
    "from snowflake.ml.registry import Registry\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f9f828-1dc4-489d-aa57-64fe8a640590",
   "metadata": {
    "language": "python",
    "name": "env_settings"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "snowflake_environment = session.sql('select current_user(), current_version()').collect()\n",
    "from snowflake.snowpark.version import VERSION\n",
    "from snowflake.ml import version\n",
    "\n",
    "# Current Environment Details\n",
    "print('User                        : {}'.format(snowflake_environment[0][0]))\n",
    "print('Role                        : {}'.format(session.get_current_role()))\n",
    "print('Database                    : {}'.format(session.get_current_database()))\n",
    "print('Schema                      : {}'.format(session.get_current_schema()))\n",
    "print('Warehouse                   : {}'.format(session.get_current_warehouse()))\n",
    "print('Snowflake version           : {}'.format(snowflake_environment[0][1]))\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(VERSION[0],VERSION[1],VERSION[2]))\n",
    "print('Snowflake ML version        : {}.{}.{}'.format(version.VERSION[0],version.VERSION[2],version.VERSION[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae356b71-d916-4433-80d3-5dc0612438ca",
   "metadata": {
    "language": "sql",
    "name": "raw_data"
   },
   "outputs": [],
   "source": [
    "select * from HR_EMPLOYEE_ATTRITION;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a75349e-87c8-44e2-a331-9acf3dde67dc",
   "metadata": {
    "language": "python",
    "name": "raw_data_dataframe"
   },
   "outputs": [],
   "source": [
    "# Load raw data from HR_EMPLOYEE_ATTRITION table\n",
    "raw_data_df = session.table(\"HR_EMPLOYEE_ATTRITION\")\n",
    "\n",
    "# We can also make references to the cells in the notebook\n",
    "# raw_data_df = raw_data.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "dataset_schema"
   },
   "outputs": [],
   "source": [
    "# Basic dataset overview using Snowpark DataFrame methods\n",
    "print(\"üìã DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get row count and column info\n",
    "row_count = raw_data_df.count()\n",
    "column_count = len(raw_data_df.columns)\n",
    "print(f\"Shape: {row_count} rows √ó {column_count} columns\")\n",
    "\n",
    "# Show column names and types\n",
    "print(f\"\\nüìä Columns and Types:\")\n",
    "for field in raw_data_df.schema.fields:\n",
    "    print(f\"   {field.name}: {field.datatype}\")\n",
    "\n",
    "print(f\"\\nüìà First 5 rows:\")\n",
    "st.dataframe(raw_data_df.limit(5))\n",
    "\n",
    "# Check for missing values and data quality using Snowpark DataFrame\n",
    "print(\"üîç DATA QUALITY CHECKS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check for duplicates by comparing total rows vs distinct rows\n",
    "total_rows = raw_data_df.count()\n",
    "distinct_rows = raw_data_df.distinct().count()\n",
    "duplicates = total_rows - distinct_rows\n",
    "print(f\"\\nüîÑ Duplicate rows: {duplicates}\")\n",
    "\n",
    "# Basic statistical summary for numerical columns\n",
    "print(f\"\\nüìä Statistical Summary (key numerical columns):\")\n",
    "numerical_stats = raw_data_df.select([\n",
    "    F.avg(\"AGE\").alias(\"avg_age\"),\n",
    "    F.min(\"AGE\").alias(\"min_age\"), \n",
    "    F.max(\"AGE\").alias(\"max_age\"),\n",
    "    F.avg(\"MONTHLY_INCOME\").alias(\"avg_income\"),\n",
    "    F.min(\"MONTHLY_INCOME\").alias(\"min_income\"),\n",
    "    F.max(\"MONTHLY_INCOME\").alias(\"max_income\"),\n",
    "    F.avg(\"YEARS_AT_COMPANY\").alias(\"avg_tenure\"),\n",
    "    F.max(\"YEARS_AT_COMPANY\").alias(\"max_tenure\")\n",
    "])\n",
    "st.dataframe(numerical_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c867a4-9af1-4419-abee-46e0020fe4ad",
   "metadata": {
    "language": "python",
    "name": "data_discovery"
   },
   "outputs": [],
   "source": [
    "# Basic Data Discovery - Check missing values and Drop unnecessary columns\n",
    "st.header(\"üîç Basic Data Discovery\")\n",
    "st.markdown(\"---\")\n",
    "\n",
    "st.write(\"Data is cleaned and have no missing value.\")\n",
    "st.write(\"Since the number of columns are overwhelming, it is plausible to retain the most crucial valuesets in relation to our analytical purposes.\")\n",
    "\n",
    "# Check for problematic columns exactly as in Medium post\n",
    "problematic_cols = ['EMPLOYEE_COUNT', 'EMPLOYEE_NUMBER', 'STANDARD_HOURS', 'OVER18', 'PERFORMANCE_RATING']\n",
    "cols_to_drop = []\n",
    "\n",
    "st.subheader(\"Columns to analyze:\")\n",
    "for col_name in problematic_cols:\n",
    "    if col_name in raw_data_df.columns:\n",
    "        unique_vals = raw_data_df.select(col_name).distinct().collect()\n",
    "        unique_count = len(unique_vals)\n",
    "        vals = [row[col_name] for row in unique_vals]\n",
    "        \n",
    "        if col_name == 'EMPLOYEE_COUNT':\n",
    "            st.write(f\"- **{col_name}**: consists of 1 value only ({vals}) - can be omitted\")\n",
    "            cols_to_drop.append(col_name)\n",
    "        elif col_name == 'OVER18':\n",
    "            st.write(f\"- **{col_name}**: consists of 1 value only ({vals}) - can be omitted\")  \n",
    "            cols_to_drop.append(col_name)\n",
    "        elif col_name == 'STANDARD_HOURS':\n",
    "            st.write(f\"- **{col_name}**: consists of 1 value only ({vals}) - can be omitted\")\n",
    "            cols_to_drop.append(col_name)\n",
    "        elif col_name == 'EMPLOYEE_NUMBER':\n",
    "            st.write(f\"- **{col_name}**: is employee ID, which is unique for each entry - keeping for reference\")\n",
    "        elif col_name == 'PERFORMANCE_RATING':\n",
    "            st.write(f\"- **{col_name}**: consists of merely {unique_count} values ({vals}) - limited analytical value\")\n",
    "            cols_to_drop.append(col_name)\n",
    "\n",
    "# Drop columns\n",
    "if cols_to_drop:\n",
    "    st.subheader(\"Drop some columns\")\n",
    "    st.write(f\"Dropping: {cols_to_drop}\")\n",
    "    cleaned_df = raw_data_df.drop(*cols_to_drop)\n",
    "    st.success(f\"Dropped {len(cols_to_drop)} columns\")\n",
    "else:\n",
    "    cleaned_df = raw_data_df\n",
    "\n",
    "# Removing Outliers\n",
    "st.subheader(\"Removing Outliers\")\n",
    "st.write(\"There are outliers in some columns, but either the number of outliers is small (<5%) or the outliers values are in realistic, reasonable range. Except for monthly_income column.\")\n",
    "\n",
    "# Calculate quartiles for monthly income\n",
    "income_stats = cleaned_df.select([\n",
    "    F.expr(\"percentile_cont(0.25) within group (order by MONTHLY_INCOME)\").alias(\"Q1\"),\n",
    "    F.expr(\"percentile_cont(0.75) within group (order by MONTHLY_INCOME)\").alias(\"Q3\"),\n",
    "    F.count(\"*\").alias(\"total_rows\")\n",
    "]).collect()[0]\n",
    "\n",
    "# Convert to float to avoid Decimal multiplication issues\n",
    "Q1 = float(income_stats['Q1'])\n",
    "Q3 = float(income_stats['Q3']) \n",
    "IQR = Q3 - Q1\n",
    "lower_bound_calc = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Income cannot be negative - use reasonable minimum instead\n",
    "import builtins\n",
    "lower_bound = builtins.max(0, lower_bound_calc)  # Ensure positive lower bound\n",
    "\n",
    "# Show outlier analysis before removal\n",
    "st.subheader(\"üìä Outlier Analysis - Monthly Income\")\n",
    "\n",
    "# Display outlier statistics\n",
    "col1, col2, col3 = st.columns(3)\n",
    "with col1:\n",
    "    st.metric(\"Q1 (25th percentile)\", f\"${Q1:,.0f}\")\n",
    "with col2:\n",
    "    st.metric(\"Q3 (75th percentile)\", f\"${Q3:,.0f}\")\n",
    "with col3:\n",
    "    st.metric(\"IQR\", f\"${IQR:,.0f}\")\n",
    "\n",
    "col1, col2 = st.columns(2)\n",
    "with col1:\n",
    "    st.metric(\"Lower Bound\", f\"${lower_bound:,.0f}\")\n",
    "with col2:\n",
    "    st.metric(\"Upper Bound\", f\"${upper_bound:,.0f}\")\n",
    "\n",
    "# Get sample data for visualization (convert small sample to pandas for plotting)\n",
    "income_sample = cleaned_df.select(\"MONTHLY_INCOME\").limit(1000).to_pandas()\n",
    "\n",
    "# Create histogram showing outliers\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.hist(income_sample['MONTHLY_INCOME'], bins=50, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "ax.axvline(lower_bound, color='red', linestyle='--', linewidth=2, label=f'Lower Bound: ${lower_bound:,.0f}')\n",
    "ax.axvline(upper_bound, color='red', linestyle='--', linewidth=2, label=f'Upper Bound: ${upper_bound:,.0f}')\n",
    "ax.axvline(Q1, color='green', linestyle='-', alpha=0.7, label=f'Q1: ${Q1:,.0f}')\n",
    "ax.axvline(Q3, color='green', linestyle='-', alpha=0.7, label=f'Q3: ${Q3:,.0f}')\n",
    "ax.set_xlabel('Monthly Income ($)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Monthly Income Distribution with Outlier Boundaries')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "st.pyplot(fig)\n",
    "plt.close()\n",
    "\n",
    "# Count and identify outliers\n",
    "outliers_df = cleaned_df.filter(\n",
    "    (F.col(\"MONTHLY_INCOME\") < lower_bound) | (F.col(\"MONTHLY_INCOME\") > upper_bound)\n",
    ")\n",
    "outlier_count = outliers_df.count()\n",
    "total_count = cleaned_df.count()\n",
    "outlier_percentage = (outlier_count / total_count) * 100\n",
    "\n",
    "st.subheader(\"üéØ Rationale for Outlier Removal\")\n",
    "\n",
    "col1, col2 = st.columns(2)\n",
    "with col1:\n",
    "    st.metric(\"Outliers Found\", f\"{outlier_count} records\")\n",
    "with col2:\n",
    "    st.metric(\"Percentage of Data\", f\"{outlier_percentage:.1f}%\")\n",
    "\n",
    "if lower_bound_calc < 0:\n",
    "    st.warning(f\"‚ö†Ô∏è Standard IQR lower bound would be ${lower_bound_calc:,.0f} (negative), adjusted to $0\")\n",
    "\n",
    "st.markdown(f\"\"\"\n",
    "**Why we're removing these outliers:**\n",
    "\n",
    "1. **Statistical Reason**: Using modified IQR method - values above ${upper_bound:,.0f} are considered outliers.\n",
    "   - Original lower bound: ${lower_bound_calc:,.0f} ‚Üí Adjusted to ${lower_bound:,.0f} (income cannot be negative)\n",
    "\n",
    "2. **Business Logic**: \n",
    "   - **Lower bound** (${lower_bound:,.0f}): Set to zero since income cannot be negative\n",
    "   - **Upper outliers** (> ${upper_bound:,.0f}): Likely executive compensation that could skew our analysis of typical employee attrition patterns\n",
    "\n",
    "3. **Model Performance**: Extreme high values can:\n",
    "   - Skew statistical measures (mean, standard deviation)\n",
    "   - Reduce the effectiveness of machine learning algorithms\n",
    "   - Make it harder to identify patterns for the majority of employees\n",
    "\n",
    "4. **Data Quality**: Only {outlier_percentage:.1f}% of the data - small enough that removal won't significantly impact our analysis but will improve model quality.\n",
    "\n",
    "**Decision**: Remove {outlier_count} outlier records to focus on typical employee compensation patterns.\n",
    "\"\"\")\n",
    "\n",
    "# Remove outliers\n",
    "final_df = cleaned_df.filter(\n",
    "    (F.col(\"MONTHLY_INCOME\") >= lower_bound) & (F.col(\"MONTHLY_INCOME\") <= upper_bound)\n",
    ")\n",
    "\n",
    "original_count = income_stats['TOTAL_ROWS']  # Snowflake uses uppercase column names\n",
    "final_count = final_df.count()\n",
    "\n",
    "st.success(f\"‚úÖ Shape of the dataset after cleaning: ({final_count}, {len(final_df.columns)})\")\n",
    "st.info(f\"üìâ Removed {original_count - final_count} outlier records from monthly_income\")\n",
    "\n",
    "# Store cleaned dataset\n",
    "cleaned_df = final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2257f785-e164-4c98-a2bc-f051c6ead4cc",
   "metadata": {
    "language": "python",
    "name": "dep_and_role_analysis"
   },
   "outputs": [],
   "source": [
    "# DEPARTMENT AND JOB ROLE ANALYSIS using Snowpark DataFrame with Interactive Streamlit Visualizations\n",
    "st.header(\"üè¢ Department & Job Role Analysis\")\n",
    "st.markdown(\"---\")\n",
    "\n",
    "# Get unique departments and job roles for dropdown options\n",
    "# Use cleaned dataset if available, otherwise use raw dataset\n",
    "analysis_df = cleaned_df if 'cleaned_df' in locals() else raw_data_df\n",
    "departments = [row['DEPARTMENT'] for row in analysis_df.select(\"DEPARTMENT\").distinct().collect()]\n",
    "job_roles = [row['JOB_ROLE'] for row in analysis_df.select(\"JOB_ROLE\").distinct().collect()]\n",
    "\n",
    "# Create side-by-side columns for the interactive charts\n",
    "col1, col2 = st.columns(2)\n",
    "\n",
    "with col1:\n",
    "    st.subheader(\"üìä Department Analysis\")\n",
    "    \n",
    "    # Department dropdown\n",
    "    selected_dept = st.selectbox(\"Select Department:\", departments, key=\"dept_selector\")\n",
    "    \n",
    "    # Filter data for selected department\n",
    "    dept_data = analysis_df.filter(F.col(\"DEPARTMENT\") == selected_dept)\n",
    "    dept_total = dept_data.count()\n",
    "    dept_attritioned = dept_data.filter(F.col(\"ATTRITION\") == \"Yes\").count()\n",
    "    dept_retained = dept_total - dept_attritioned\n",
    "    dept_attrition_rate = (dept_attritioned / dept_total) * 100 if dept_total > 0 else 0\n",
    "    \n",
    "    # Display statistics\n",
    "    st.metric(\n",
    "        label=\"Attrition Rate\", \n",
    "        value=f\"{dept_attrition_rate:.1f}%\",\n",
    "        delta=f\"{dept_attritioned} out of {dept_total} employees\"\n",
    "    )\n",
    "    \n",
    "    # Create pie chart for department\n",
    "    if dept_total > 0:\n",
    "        fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "        labels = ['Retained', 'Left Company']\n",
    "        sizes = [dept_retained, dept_attritioned]\n",
    "        colors = ['lightblue', 'salmon']\n",
    "        explode = (0.05, 0.05)  # slightly separate the slices\n",
    "        \n",
    "        ax1.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors, \n",
    "                explode=explode, shadow=True, startangle=90)\n",
    "        ax1.set_title(f'Attrition in {selected_dept}', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        st.pyplot(fig1)\n",
    "        plt.close()\n",
    "    else:\n",
    "        st.warning(\"No data available for selected department\")\n",
    "\n",
    "with col2:\n",
    "    st.subheader(\"üëî Job Role Analysis\")\n",
    "    \n",
    "    # Job role dropdown\n",
    "    selected_role = st.selectbox(\"Select Job Role:\", job_roles, key=\"role_selector\")\n",
    "    \n",
    "    # Filter data for selected job role\n",
    "    role_data = analysis_df.filter(F.col(\"JOB_ROLE\") == selected_role)\n",
    "    role_total = role_data.count()\n",
    "    role_attritioned = role_data.filter(F.col(\"ATTRITION\") == \"Yes\").count()\n",
    "    role_retained = role_total - role_attritioned\n",
    "    role_attrition_rate = (role_attritioned / role_total) * 100 if role_total > 0 else 0\n",
    "    \n",
    "    # Display statistics\n",
    "    st.metric(\n",
    "        label=\"Attrition Rate\", \n",
    "        value=f\"{role_attrition_rate:.1f}%\",\n",
    "        delta=f\"{role_attritioned} out of {role_total} employees\"\n",
    "    )\n",
    "    \n",
    "    # Create pie chart for job role\n",
    "    if role_total > 0:\n",
    "        fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "        labels = ['Retained', 'Left Company']\n",
    "        sizes = [role_retained, role_attritioned]\n",
    "        colors = ['lightgreen', 'orange']\n",
    "        explode = (0.05, 0.05)  # slightly separate the slices\n",
    "        \n",
    "        ax2.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors, \n",
    "                explode=explode, shadow=True, startangle=90)\n",
    "        ax2.set_title(f'Attrition for {selected_role}', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        st.pyplot(fig2)\n",
    "        plt.close()\n",
    "    else:\n",
    "        st.warning(\"No data available for selected job role\")\n",
    "\n",
    "# Summary table showing all departments and roles\n",
    "st.subheader(\"üìà Summary Tables\")\n",
    "\n",
    "# Create tabs for detailed breakdown\n",
    "tab1, tab2 = st.tabs([\"Department Breakdown\", \"Job Role Breakdown\"])\n",
    "\n",
    "with tab1:\n",
    "    # Department analysis table using Snowpark DataFrame\n",
    "    dept_analysis = analysis_df.group_by(\"DEPARTMENT\").agg([\n",
    "        F.count(\"*\").alias(\"total_employees\"),\n",
    "        F.sum(F.when(F.col(\"ATTRITION\") == \"Yes\", 1).otherwise(0)).alias(\"attritioned\"),\n",
    "        F.avg(F.when(F.col(\"ATTRITION\") == \"Yes\", 1).otherwise(0)).alias(\"attrition_rate_decimal\")\n",
    "    ]).with_column(\"attrition_rate_pct\", F.col(\"attrition_rate_decimal\") * 100).order_by(F.col(\"attrition_rate_pct\").desc())\n",
    "    \n",
    "    st.dataframe(dept_analysis, use_container_width=True)\n",
    "\n",
    "with tab2:\n",
    "    # Job role analysis table using Snowpark DataFrame\n",
    "    role_analysis = analysis_df.group_by(\"JOB_ROLE\").agg([\n",
    "        F.count(\"*\").alias(\"total_employees\"),\n",
    "        F.sum(F.when(F.col(\"ATTRITION\") == \"Yes\", 1).otherwise(0)).alias(\"attritioned\"),\n",
    "        F.avg(F.when(F.col(\"ATTRITION\") == \"Yes\", 1).otherwise(0)).alias(\"attrition_rate_decimal\")\n",
    "    ]).with_column(\"attrition_rate_pct\", F.col(\"attrition_rate_decimal\") * 100).order_by(F.col(\"attrition_rate_pct\").desc())\n",
    "    \n",
    "    st.dataframe(role_analysis, use_container_width=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fcf241-c62a-4936-9d4f-5829a6b0d01c",
   "metadata": {
    "language": "python",
    "name": "overtime_analy"
   },
   "outputs": [],
   "source": [
    "# OVERTIME ANALYSIS using Snowpark DataFrame\n",
    "print(\"‚è∞ OVERTIME ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Overtime impact on attrition using Snowpark DataFrame\n",
    "overtime_analysis = raw_data_df.group_by(\"OVER_TIME\").agg([\n",
    "    F.count(\"*\").alias(\"total_employees\"),\n",
    "    F.sum(F.when(F.col(\"ATTRITION\") == \"Yes\", 1).otherwise(0)).alias(\"attritioned\"),\n",
    "    F.avg(F.when(F.col(\"ATTRITION\") == \"Yes\", 1).otherwise(0)).alias(\"attrition_rate_decimal\")\n",
    "]).with_column(\"attrition_rate_pct\", F.col(\"attrition_rate_decimal\") * 100).order_by(\"OVER_TIME\")\n",
    "\n",
    "print(\"üìä Attrition by Overtime Status:\")\n",
    "overtime_results = overtime_analysis.collect()\n",
    "overtime_dict = {}\n",
    "for row in overtime_results:\n",
    "    status = row['OVER_TIME']\n",
    "    total = row['TOTAL_EMPLOYEES']\n",
    "    attritioned = row['ATTRITIONED']\n",
    "    rate = row['ATTRITION_RATE_PCT']\n",
    "    print(f\"  {status}: {total} total, {attritioned} left ({rate:.1f}%)\")\n",
    "    overtime_dict[status] = rate\n",
    "\n",
    "# Calculate the difference if we have both Yes and No\n",
    "if 'Yes' in overtime_dict and 'No' in overtime_dict:\n",
    "    overtime_diff = overtime_dict['Yes'] - overtime_dict['No']\n",
    "    print(f\"\\nüìà Overtime Impact: +{overtime_diff:.1f} percentage points\")\n",
    "    print(f\"üéØ Expected from Medium article: ~20 percentage points difference\")\n",
    "    print(f\"   (30.5% for overtime vs 10.4% for non-overtime)\")\n",
    "\n",
    "# Visualization using collected data\n",
    "plt.figure(figsize=(10, 6))\n",
    "overtime_status = list(overtime_dict.keys())\n",
    "overtime_rates = list(overtime_dict.values())\n",
    "\n",
    "plt.bar(overtime_status, overtime_rates, color=['mediumblue', 'lightblue'])\n",
    "plt.title('Attrition Rate by Overtime Status', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Overtime Status')\n",
    "plt.ylabel('Attrition Rate (%)')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b0ca90-adeb-41b8-a42f-3ae13915cf69",
   "metadata": {
    "language": "python",
    "name": "heatmap"
   },
   "outputs": [],
   "source": [
    "# 1. CORRELATION HEATMAP ANALYSIS\n",
    "st.subheader(\"üî• Feature Correlation Heatmap\")\n",
    "st.markdown(\"*Analyzing relationships between numerical features to identify patterns*\")\n",
    "\n",
    "# Get numerical columns for correlation analysis\n",
    "# Exclude EMPLOYEE_NUMBER if it exists, and get sample for correlation calculation\n",
    "numerical_columns = []\n",
    "for field in cleaned_df.schema.fields:\n",
    "    # Include numerical columns but exclude EMPLOYEE_NUMBER and ID-like fields  \n",
    "    datatype_str = str(field.datatype)\n",
    "    if (any(num_type in datatype_str for num_type in ['LongType', 'IntegerType', 'FloatType', 'DoubleType', 'DecimalType']) and \n",
    "        field.name not in ['EMPLOYEE_NUMBER', 'EMPLOYEE_COUNT']):\n",
    "        numerical_columns.append(field.name)\n",
    "\n",
    "if numerical_columns:\n",
    "    # Get sample data for correlation calculation (using pandas for correlation matrix)\n",
    "    st.info(f\"üìà Analyzing correlations for {len(numerical_columns)} numerical features\")\n",
    "    \n",
    "    # Sample data for correlation (limit to reasonable size for performance)\n",
    "    correlation_sample = cleaned_df.select(*numerical_columns).limit(1000).to_pandas()\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = correlation_sample.corr()\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig, ax = plt.subplots(figsize=(14, 12))\n",
    "    \n",
    "    # Generate heatmap with better styling\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # Mask upper triangle\n",
    "    sns.heatmap(correlation_matrix, \n",
    "                mask=mask,\n",
    "                annot=True, \n",
    "                cmap='RdYlBu_r', \n",
    "                center=0,\n",
    "                square=True,\n",
    "                fmt='.2f', \n",
    "                cbar_kws={\"shrink\": .8},\n",
    "                ax=ax)\n",
    "    \n",
    "    ax.set_title('Feature Correlation Heatmap\\n(Lower Triangle Only)', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    st.pyplot(fig)\n",
    "    plt.close()\n",
    "    \n",
    "    # Find highly correlated features (absolute correlation > 0.7)\n",
    "    st.subheader(\"üîç Highly Correlated Feature Pairs\")\n",
    "    high_corr_pairs = []\n",
    "    import builtins  # Import builtins to access Python's built-in abs function\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_value = correlation_matrix.iloc[i, j]\n",
    "            if builtins.abs(corr_value) > 0.7:\n",
    "                high_corr_pairs.append({\n",
    "                    'Feature 1': correlation_matrix.columns[i],\n",
    "                    'Feature 2': correlation_matrix.columns[j],\n",
    "                    'Correlation': corr_value\n",
    "                })\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        corr_df = pd.DataFrame(high_corr_pairs)\n",
    "        corr_df = corr_df.reindex(corr_df['Correlation'].abs().sort_values(ascending=False).index)\n",
    "        st.dataframe(corr_df, use_container_width=True)\n",
    "    else:\n",
    "        st.info(\"No highly correlated feature pairs found (threshold: |correlation| > 0.7)\")\n",
    "else:\n",
    "    st.warning(\"No numerical columns found for correlation analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92fdff4-ad75-42ce-b437-85dd0cbfe927",
   "metadata": {
    "language": "python",
    "name": "demographic_analysis"
   },
   "outputs": [],
   "source": [
    "# 2. GENDER AND AGE DISTRIBUTIONS\n",
    "st.subheader(\"üë• Gender & Age Distribution Analysis\")\n",
    "st.markdown(\"*Understanding demographic patterns in our workforce*\")\n",
    "\n",
    "# Create side-by-side analysis\n",
    "col1, col2 = st.columns(2)\n",
    "\n",
    "with col1:\n",
    "    st.markdown(\"**Gender Distribution**\")\n",
    "    \n",
    "    # Gender distribution using Snowpark DataFrame\n",
    "    gender_dist = cleaned_df.group_by(\"GENDER\").agg([\n",
    "        F.count(\"*\").alias(\"count\"),\n",
    "        F.avg(F.when(F.col(\"ATTRITION\") == \"Yes\", 1).otherwise(0)).alias(\"attrition_rate\")\n",
    "    ]).with_column(\"attrition_rate_pct\", F.col(\"attrition_rate\") * 100)\n",
    "    \n",
    "    gender_results = gender_dist.collect()\n",
    "    \n",
    "    # Prepare data for visualization\n",
    "    genders = [row['GENDER'] for row in gender_results]\n",
    "    gender_counts = [row['COUNT'] for row in gender_results]\n",
    "    gender_attrition_rates = [row['ATTRITION_RATE_PCT'] for row in gender_results]\n",
    "    \n",
    "    # Create gender distribution pie chart\n",
    "    fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "    colors = ['lightblue', 'lightpink']\n",
    "    ax1.pie(gender_counts, labels=genders, autopct='%1.1f%%', colors=colors, \n",
    "            startangle=90, explode=(0.05, 0.05))\n",
    "    ax1.set_title('Employee Distribution by Gender', fontsize=12, fontweight='bold')\n",
    "    st.pyplot(fig1)\n",
    "    plt.close()\n",
    "    \n",
    "    # Display gender attrition rates\n",
    "    st.markdown(\"**Attrition Rate by Gender:**\")\n",
    "    for i, gender in enumerate(genders):\n",
    "        st.write(f\"- {gender}: {gender_attrition_rates[i]:.1f}%\")\n",
    "\n",
    "with col2:\n",
    "    st.markdown(\"**Age Distribution**\")\n",
    "    \n",
    "    # Get age statistics using Snowpark\n",
    "    age_stats = cleaned_df.select([\n",
    "        F.min(\"AGE\").alias(\"min_age\"),\n",
    "        F.max(\"AGE\").alias(\"max_age\"),\n",
    "        F.avg(\"AGE\").alias(\"avg_age\"),\n",
    "        F.expr(\"percentile_cont(0.25) within group (order by AGE)\").alias(\"Q1\"),\n",
    "        F.expr(\"percentile_cont(0.50) within group (order by AGE)\").alias(\"median\"),\n",
    "        F.expr(\"percentile_cont(0.75) within group (order by AGE)\").alias(\"Q3\")\n",
    "    ]).collect()[0]\n",
    "    \n",
    "    # Display age statistics\n",
    "    st.metric(\"Average Age\", f\"{age_stats['AVG_AGE']:.1f} years\")\n",
    "    st.metric(\"Age Range\", f\"{age_stats['MIN_AGE']} - {age_stats['MAX_AGE']} years\")\n",
    "    st.metric(\"Median Age\", f\"{age_stats['MEDIAN']:.0f} years\")\n",
    "    \n",
    "    # Get age distribution data for histogram\n",
    "    age_sample = cleaned_df.select(\"AGE\", \"ATTRITION\").limit(1000).to_pandas()\n",
    "    \n",
    "    # Create age distribution histogram\n",
    "    fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "    ax2.hist(age_sample['AGE'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax2.axvline(age_stats['AVG_AGE'], color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {age_stats[\"AVG_AGE\"]:.1f}')\n",
    "    ax2.axvline(age_stats['MEDIAN'], color='green', linestyle='--', linewidth=2, \n",
    "                label=f'Median: {age_stats[\"MEDIAN\"]:.0f}')\n",
    "    ax2.set_xlabel('Age')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Age Distribution of Employees')\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    st.pyplot(fig2)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e44bbde-5901-4430-bd25-d14b9053e6d4",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "demographic_analysis2"
   },
   "outputs": [],
   "source": [
    "# 3. ATTRITION VS DENSITY BY GENDER\n",
    "st.subheader(\"‚öñÔ∏è Attrition Density Analysis by Gender\")\n",
    "st.markdown(\"*Comparing attrition patterns between male and female employees*\")\n",
    "\n",
    "# Get gender-specific attrition data using Snowpark\n",
    "gender_attrition_data = cleaned_df.select(\"GENDER\", \"AGE\", \"ATTRITION\", \"MONTHLY_INCOME\").to_pandas()\n",
    "\n",
    "# Create density plots for age by gender and attrition\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Age density by gender and attrition\n",
    "for i, gender in enumerate(['Male', 'Female']):\n",
    "    gender_data = gender_attrition_data[gender_attrition_data['GENDER'] == gender]\n",
    "    \n",
    "    # Age density plot\n",
    "    ax = axes[0, i]\n",
    "    stayed = gender_data[gender_data['ATTRITION'] == 'No']['AGE']\n",
    "    left = gender_data[gender_data['ATTRITION'] == 'Yes']['AGE']\n",
    "    \n",
    "    ax.hist(stayed, bins=20, alpha=0.7, label='Stayed', color='lightblue', density=True)\n",
    "    ax.hist(left, bins=20, alpha=0.7, label='Left', color='salmon', density=True)\n",
    "    ax.set_title(f'Age Distribution - {gender}')\n",
    "    ax.set_xlabel('Age')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Income density by gender and attrition  \n",
    "for i, gender in enumerate(['Male', 'Female']):\n",
    "    gender_data = gender_attrition_data[gender_attrition_data['GENDER'] == gender]\n",
    "    \n",
    "    # Income density plot\n",
    "    ax = axes[1, i]\n",
    "    stayed = gender_data[gender_data['ATTRITION'] == 'No']['MONTHLY_INCOME']\n",
    "    left = gender_data[gender_data['ATTRITION'] == 'Yes']['MONTHLY_INCOME']\n",
    "    \n",
    "    ax.hist(stayed, bins=20, alpha=0.7, label='Stayed', color='lightgreen', density=True)\n",
    "    ax.hist(left, bins=20, alpha=0.7, label='Left', color='orange', density=True)\n",
    "    ax.set_title(f'Income Distribution - {gender}')\n",
    "    ax.set_xlabel('Monthly Income ($)')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "st.pyplot(fig)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ceaae-8c98-4752-be8f-402a5ea7dca7",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "income_attrition"
   },
   "outputs": [],
   "source": [
    "# MONTHLY INCOME CORRELATION WITH ATTRITION  \n",
    "st.subheader(\"üí∞ Monthly Income Correlation with Attrition\")  \n",
    "st.markdown(\"*Analyzing how monthly income levels correlate with employee attrition rates*\")  \n",
    "# Get income and attrition data using Snowpark  \n",
    "income_attrition_data = cleaned_df.select(\"MONTHLY_INCOME\", \"ATTRITION\").to_pandas()  \n",
    "# Create income analysis visualizations  \n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))  \n",
    "# Income distribution by attrition status  \n",
    "ax = axes[0, 0]  \n",
    "stayed = income_attrition_data[income_attrition_data['ATTRITION'] == 'No']['MONTHLY_INCOME']  \n",
    "left = income_attrition_data[income_attrition_data['ATTRITION'] == 'Yes']['MONTHLY_INCOME']  \n",
    "ax.hist(stayed, bins=30, alpha=0.7, label='Stayed', color='lightblue', density=True)  \n",
    "ax.hist(left, bins=30, alpha=0.7, label='Left', color='salmon', density=True)  \n",
    "ax.set_title('Monthly Income Distribution by Attrition')  \n",
    "ax.set_xlabel('Monthly Income ($)')  \n",
    "ax.set_ylabel('Density')  \n",
    "ax.legend()  \n",
    "ax.grid(axis='y', alpha=0.3)  \n",
    "# Box plot comparison  \n",
    "ax = axes[0, 1]  \n",
    "income_attrition_data.boxplot(column='MONTHLY_INCOME', by='ATTRITION', ax=ax)  \n",
    "ax.set_title('Monthly Income Box Plot by Attrition Status')  \n",
    "ax.set_xlabel('Attrition Status')  \n",
    "ax.set_ylabel('Monthly Income ($)')  \n",
    "# Income bins analysis  \n",
    "income_attrition_data['INCOME_BIN'] = pd.cut(income_attrition_data['MONTHLY_INCOME'],   \n",
    "                                           bins=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])  \n",
    "# Attrition rate by income bin  \n",
    "ax = axes[1, 0]  \n",
    "attrition_by_income = income_attrition_data.groupby('INCOME_BIN')['ATTRITION'].apply(  \n",
    "    lambda x: (x == 'Yes').sum() / len(x) * 100  \n",
    ").reset_index()  \n",
    "attrition_by_income.columns = ['INCOME_BIN', 'ATTRITION_RATE']  \n",
    "bars = ax.bar(attrition_by_income['INCOME_BIN'], attrition_by_income['ATTRITION_RATE'],   \n",
    "              color='coral', alpha=0.7)  \n",
    "ax.set_title('Attrition Rate by Income Level')  \n",
    "ax.set_xlabel('Income Level')  \n",
    "ax.set_ylabel('Attrition Rate (%)')  \n",
    "ax.grid(axis='y', alpha=0.3)  \n",
    "# Add value labels on bars  \n",
    "for bar in bars:  \n",
    "    height = bar.get_height()  \n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,  \n",
    "            f'{height:.1f}%', ha='center', va='bottom')  \n",
    "# Count by income bin and attrition  \n",
    "ax = axes[1, 1]  \n",
    "income_counts = income_attrition_data.groupby(['INCOME_BIN', 'ATTRITION']).size().unstack()  \n",
    "income_counts.plot(kind='bar', ax=ax, color=['lightblue', 'salmon'], alpha=0.7)  \n",
    "ax.set_title('Employee Count by Income Level and Attrition')  \n",
    "ax.set_xlabel('Income Level')  \n",
    "ax.set_ylabel('Employee Count')  \n",
    "ax.legend(title='Attrition')  \n",
    "ax.grid(axis='y', alpha=0.3)  \n",
    "plt.setp(ax.get_xticklabels(), rotation=45)  \n",
    "plt.tight_layout()  \n",
    "st.pyplot(fig)  \n",
    "plt.close()  \n",
    "# Statistical summary  \n",
    "st.subheader(\"üìà Income-Attrition Statistical Summary\")  \n",
    "col1, col2 = st.columns(2)  \n",
    "with col1:  \n",
    "    st.write(\"**Average Monthly Income by Attrition Status:**\")  \n",
    "    avg_income = income_attrition_data.groupby('ATTRITION')['MONTHLY_INCOME'].agg(['mean', 'median', 'std'])  \n",
    "    st.dataframe(avg_income.round(2))  \n",
    "with col2:  \n",
    "    st.write(\"**Attrition Rate by Income Quartile:**\")  \n",
    "    income_attrition_data['INCOME_QUARTILE'] = pd.qcut(income_attrition_data['MONTHLY_INCOME'],   \n",
    "                                                      q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])  \n",
    "    quartile_attrition = income_attrition_data.groupby('INCOME_QUARTILE')['ATTRITION'].apply(  \n",
    "        lambda x: (x == 'Yes').sum() / len(x) * 100  \n",
    "    ).round(2)  \n",
    "    st.dataframe(quartile_attrition.to_frame('Attrition Rate (%)'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cb561e-86d9-492b-aa1f-cd85911d57f7",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "position_attrition"
   },
   "outputs": [],
   "source": [
    "# 5. ATTRITION RATE BY POSITION (JOB ROLE)\n",
    "st.subheader(\"üéØ Attrition Rate by Position\")\n",
    "st.markdown(\"*Identifying which job roles have the highest turnover risk*\")\n",
    "\n",
    "# Job role attrition analysis using Snowpark\n",
    "job_role_analysis = cleaned_df.group_by(\"JOB_ROLE\").agg([\n",
    "    F.count(\"*\").alias(\"total_employees\"),\n",
    "    F.sum(F.when(F.col(\"ATTRITION\") == \"Yes\", 1).otherwise(0)).alias(\"attritioned\"),\n",
    "    F.avg(F.when(F.col(\"ATTRITION\") == \"Yes\", 1).otherwise(0)).alias(\"attrition_rate_decimal\")\n",
    "]).with_column(\"attrition_rate_pct\", F.col(\"attrition_rate_decimal\") * 100)\\\n",
    "  .filter(F.col(\"total_employees\") >= 5)\\\n",
    "  .order_by(F.col(\"attrition_rate_pct\").desc())\n",
    "\n",
    "job_role_results = job_role_analysis.collect()\n",
    "\n",
    "# Display top positions with highest attrition\n",
    "col1, col2 = st.columns([2, 1])\n",
    "\n",
    "with col1:\n",
    "    # Create horizontal bar chart for better readability\n",
    "    positions = [row['JOB_ROLE'] for row in job_role_results[:10]]  # Top 10\n",
    "    attrition_rates = [row['ATTRITION_RATE_PCT'] for row in job_role_results[:10]]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    bars = ax.barh(positions, attrition_rates, color='salmon')\n",
    "    ax.set_xlabel('Attrition Rate (%)')\n",
    "    ax.set_title('Top 10 Positions by Attrition Rate', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    for bar, rate in zip(bars, attrition_rates):\n",
    "        width = bar.get_width()\n",
    "        ax.text(width + 0.5, bar.get_y() + bar.get_height()/2, \n",
    "                f'{rate:.1f}%', ha='left', va='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    st.pyplot(fig)\n",
    "    plt.close()\n",
    "\n",
    "with col2:\n",
    "    st.markdown(\"**üìä Position Risk Summary**\")\n",
    "    for i, row in enumerate(job_role_results[:5]):  # Top 5 highest risk\n",
    "        role = row['JOB_ROLE']\n",
    "        rate = row['ATTRITION_RATE_PCT']\n",
    "        total = row['TOTAL_EMPLOYEES']\n",
    "        attritioned = row['ATTRITIONED']\n",
    "        \n",
    "        st.write(f\"**{i+1}. {role}**\")\n",
    "        st.write(f\"   Rate: {rate:.1f}%\")\n",
    "        st.write(f\"   ({attritioned}/{total} employees)\")\n",
    "        st.write(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9e2566-5d71-465c-a975-af09771d5ef2",
   "metadata": {
    "language": "python",
    "name": "job_sat_attrition"
   },
   "outputs": [],
   "source": [
    "# 6. JOB SATISFACTION ANALYSIS\n",
    "st.subheader(\"üòä Job Satisfaction Analysis\")\n",
    "st.markdown(\"*Understanding the relationship between job satisfaction and employee retention*\")\n",
    "\n",
    "# Job satisfaction analysis using Snowpark DataFrame\n",
    "satisfaction_analysis = cleaned_df.group_by(\"JOB_SATISFACTION\").agg([\n",
    "    F.count(\"*\").alias(\"total_employees\"),\n",
    "    F.sum(F.when(F.col(\"ATTRITION\") == \"Yes\", 1).otherwise(0)).alias(\"attritioned\"),\n",
    "    F.avg(F.when(F.col(\"ATTRITION\") == \"Yes\", 1).otherwise(0)).alias(\"attrition_rate_decimal\")\n",
    "]).with_column(\"attrition_rate_pct\", F.col(\"attrition_rate_decimal\") * 100)\\\n",
    "  .order_by(\"JOB_SATISFACTION\")\n",
    "\n",
    "satisfaction_results = satisfaction_analysis.collect()\n",
    "\n",
    "# Create side-by-side analysis\n",
    "col1, col2 = st.columns(2)\n",
    "\n",
    "with col1:\n",
    "    st.markdown(\"**üìä Satisfaction Levels Distribution**\")\n",
    "    \n",
    "    # Satisfaction level distribution\n",
    "    satisfaction_levels = [row['JOB_SATISFACTION'] for row in satisfaction_results]\n",
    "    satisfaction_counts = [row['TOTAL_EMPLOYEES'] for row in satisfaction_results]\n",
    "    \n",
    "    fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "    bars = ax1.bar(satisfaction_levels, satisfaction_counts, color='lightblue', alpha=0.8)\n",
    "    ax1.set_xlabel('Job Satisfaction Level')\n",
    "    ax1.set_ylabel('Number of Employees')\n",
    "    ax1.set_title('Employee Distribution by Job Satisfaction Level')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for bar, count in zip(bars, satisfaction_counts):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    st.pyplot(fig1)\n",
    "    plt.close()\n",
    "\n",
    "with col2:\n",
    "    st.markdown(\"**‚ö†Ô∏è Attrition Rate by Satisfaction Level**\")\n",
    "    \n",
    "    # Attrition rate by satisfaction level\n",
    "    attrition_rates = [row['ATTRITION_RATE_PCT'] for row in satisfaction_results]\n",
    "    \n",
    "    fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "    bars = ax2.bar(satisfaction_levels, attrition_rates, color='salmon', alpha=0.8)\n",
    "    ax2.set_xlabel('Job Satisfaction Level')\n",
    "    ax2.set_ylabel('Attrition Rate (%)')\n",
    "    ax2.set_title('Attrition Rate by Job Satisfaction Level')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    for bar, rate in zip(bars, attrition_rates):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    st.pyplot(fig2)\n",
    "    plt.close()\n",
    "\n",
    "# Detailed satisfaction analysis table\n",
    "st.subheader(\"üìã Detailed Job Satisfaction Analysis\")\n",
    "\n",
    "# Create a more readable table\n",
    "satisfaction_display = []\n",
    "for row in satisfaction_results:\n",
    "    satisfaction_display.append({\n",
    "        'Satisfaction Level': f\"Level {row['JOB_SATISFACTION']}\",\n",
    "        'Total Employees': row['TOTAL_EMPLOYEES'],\n",
    "        'Employees Who Left': row['ATTRITIONED'],\n",
    "        'Attrition Rate (%)': f\"{row['ATTRITION_RATE_PCT']:.1f}%\"\n",
    "    })\n",
    "satisfaction_df = pd.DataFrame(satisfaction_display)\n",
    "st.dataframe(satisfaction_df, use_container_width=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86a208f-a59b-4019-94fb-54648ee1cbcb",
   "metadata": {
    "language": "python",
    "name": "pairwise_corr"
   },
   "outputs": [],
   "source": [
    "# 7. PAIRWISE PLOTS FOR KEY FEATURES  \n",
    "st.subheader(\"üîó Pairwise Relationships Analysis\")\n",
    "# Define specific columns for pairwise analysis (matching your sample code)\n",
    "cols = ['TOTAL_WORKING_YEARS', 'YEARS_AT_COMPANY', 'YEARS_IN_CURRENT_ROLE', \n",
    "        'YEARS_SINCE_LAST_PROMOTION', 'ATTRITION', 'JOB_LEVEL']\n",
    "# Check which features exist in our dataset\n",
    "available_features = []\n",
    "for feature in cols:\n",
    "    if feature in [field.name for field in cleaned_df.schema.fields]:\n",
    "        available_features.append(feature)\n",
    "\n",
    "if len(available_features) >= 3:\n",
    "    st.info(f\"üìä Creating pairwise plots for: {', '.join(available_features)}\")\n",
    "    \n",
    "    # Get sample data for pairwise plotting (pandas required for seaborn pairplot)\n",
    "    pairwise_sample = cleaned_df.select(*available_features).limit(500).to_pandas()\n",
    "    \n",
    "    # Create pairwise plot with seaborn - simple approach matching your sample\n",
    "    st.subheader(\"üìà Pairwise Feature Relationships\")\n",
    "    \n",
    "    # Use seaborn pairplot with hue for attrition (simple approach)\n",
    "    pair_plot = sns.pairplot(pairwise_sample, hue='ATTRITION')\n",
    "    \n",
    "    # Customize the plot\n",
    "    pair_plot.fig.suptitle('Pairwise Relationships - Tenure Features vs Attrition', \n",
    "                          fontsize=16, fontweight='bold', y=1.02)\n",
    "    \n",
    "    st.pyplot(pair_plot.fig)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bb1ae1-7fcf-4f63-bb75-f6f33adec7ac",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "# # ========================================\n",
    "# # FEATURE ENGINEERING & MODEL PREPARATION\n",
    "# # ========================================\n",
    "# st.header(\"üõ†Ô∏è Feature Engineering & Model Preparation\")\n",
    "# st.markdown(\"---\")\n",
    "# st.markdown(\"*Preparing data for machine learning following Snowflake ML best practices*\")\n",
    "\n",
    "# # Display current dataset info\n",
    "# total_rows = cleaned_df.count()\n",
    "# total_cols = len(cleaned_df.columns)\n",
    "# st.info(f\"üìä **Starting Dataset**: {total_rows:,} rows √ó {total_cols} columns\")\n",
    "\n",
    "# # Check for EMPLOYEE_NUMBER column (to exclude from modeling)\n",
    "# schema_fields = [field.name for field in cleaned_df.schema.fields]\n",
    "# if 'EMPLOYEE_NUMBER' in schema_fields:\n",
    "#     st.warning(\"üìã **Note**: EMPLOYEE_NUMBER will be kept for reference but excluded from model training\")\n",
    "\n",
    "# print(\"üöÄ Starting Feature Engineering Pipeline...\")\n",
    "# print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9ea494-258c-40ff-bcd0-2ecc952655da",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": true,
    "language": "python",
    "name": "feature_engineering1"
   },
   "outputs": [],
   "source": [
    "# Feature engineering pipeline\n",
    "print(\"Starting feature engineering...\")\n",
    "\n",
    "# Define column types for encoding\n",
    "ordinal_columns = [\n",
    "    'EDUCATION', 'ENVIRONMENT_SATISFACTION', 'JOB_LEVEL',\n",
    "    'JOB_SATISFACTION', 'RELATIONSHIP_SATISFACTION', 'WORK_LIFE_BALANCE'\n",
    "]\n",
    "\n",
    "# Categorize columns\n",
    "categorical_columns = []\n",
    "ordinal_columns_present = []\n",
    "numerical_columns = []\n",
    "target_column = 'ATTRITION'\n",
    "exclude_columns = ['EMPLOYEE_NUMBER'] if 'EMPLOYEE_NUMBER' in [f.name for f in cleaned_df.schema.fields] else []\n",
    "\n",
    "# Analyze columns and categorize\n",
    "for field in cleaned_df.schema.fields:\n",
    "    col_name = field.name\n",
    "    datatype_str = str(field.datatype)\n",
    "    \n",
    "    if col_name == target_column or col_name in exclude_columns:\n",
    "        continue\n",
    "    \n",
    "    if col_name in ordinal_columns:\n",
    "        ordinal_columns_present.append(col_name)\n",
    "    elif any(num_type in datatype_str for num_type in ['LongType', 'IntegerType', 'FloatType', 'DoubleType', 'DecimalType']):\n",
    "        unique_count = cleaned_df.select(col_name).distinct().count()\n",
    "        if unique_count <= 10 and col_name not in ordinal_columns:\n",
    "            categorical_columns.append(col_name)\n",
    "        else:\n",
    "            numerical_columns.append(col_name)\n",
    "    else:\n",
    "        categorical_columns.append(col_name)\n",
    "\n",
    "print(f\"Ordinal columns: {len(ordinal_columns_present)}\")\n",
    "print(f\"Categorical columns: {len(categorical_columns)}\")\n",
    "print(f\"Numerical columns: {len(numerical_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f344a5-9dc7-4c77-8e05-27a021710788",
   "metadata": {
    "language": "python",
    "name": "feature_engineering2"
   },
   "outputs": [],
   "source": [
    "# Apply feature encoding\n",
    "feature_df = cleaned_df\n",
    "\n",
    "# Ordinal encoding\n",
    "if ordinal_columns_present:\n",
    "    print(\"Applying ordinal encoding...\")\n",
    "    ordinal_encoder = OrdinalEncoder(\n",
    "        input_cols=ordinal_columns_present,\n",
    "        output_cols=[f\"{col}_ORDINAL\" for col in ordinal_columns_present]\n",
    "    )\n",
    "    ordinal_encoder.fit(feature_df)\n",
    "    feature_df = ordinal_encoder.transform(feature_df)\n",
    "    feature_df = feature_df.drop(*ordinal_columns_present)\n",
    "    ordinal_encoded_columns = [f\"{col}_ORDINAL\" for col in ordinal_columns_present]\n",
    "    print(f\"Ordinal encoded: {len(ordinal_encoded_columns)} columns\")\n",
    "else:\n",
    "    ordinal_encoded_columns = []\n",
    "\n",
    "# One-hot encoding\n",
    "if categorical_columns:\n",
    "    print(\"Applying one-hot encoding...\")\n",
    "    ohe = OneHotEncoder(\n",
    "        input_cols=categorical_columns,\n",
    "        output_cols=[f\"{col}_ONEHOT\" for col in categorical_columns]\n",
    "    )\n",
    "    ohe.fit(feature_df)\n",
    "    feature_df = ohe.transform(feature_df)\n",
    "    feature_df = feature_df.drop(*categorical_columns)\n",
    "    onehot_encoded_columns = [f\"{col}_ONEHOT\" for col in categorical_columns]\n",
    "    print(f\"One-hot encoded: {len(onehot_encoded_columns)} columns\")\n",
    "else:\n",
    "    onehot_encoded_columns = []\n",
    "\n",
    "# Standard scaling for numerical features\n",
    "if numerical_columns:\n",
    "    print(\"Applying standard scaling...\")\n",
    "    scaler = StandardScaler(\n",
    "        input_cols=numerical_columns,\n",
    "        output_cols=[f\"{col}_SCALED\" for col in numerical_columns]\n",
    "    )\n",
    "    scaler.fit(feature_df)\n",
    "    feature_df = scaler.transform(feature_df)\n",
    "    feature_df = feature_df.drop(*numerical_columns)\n",
    "    scaled_columns = [f\"{col}_SCALED\" for col in numerical_columns]\n",
    "    print(f\"Scaled: {len(scaled_columns)} columns\")\n",
    "else:\n",
    "    scaled_columns = []\n",
    "\n",
    "print(\"Feature engineering complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9a2424-8b6c-4dd1-a852-0556c7bd8467",
   "metadata": {
    "language": "python",
    "name": "train_test_split"
   },
   "outputs": [],
   "source": [
    "# Convert ATTRITION to numeric (0/1) for ML\n",
    "from snowflake.snowpark.functions import col, when\n",
    "feature_df = feature_df.with_column(\"ATTRITION\", \n",
    "    when(col(\"ATTRITION\") == \"Yes\", 1).otherwise(0).cast(LongType()))\n",
    "\n",
    "# Train/test split (80/20)\n",
    "print(\"Creating train/test split...\")\n",
    "train_df, test_df = feature_df.random_split(weights=[0.8, 0.2], seed=42)\n",
    "\n",
    "train_count = train_df.count()\n",
    "test_count = test_df.count()\n",
    "print(f\"Training set: {train_count} samples\")\n",
    "print(f\"Test set: {test_count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddccd04-b20e-4238-992a-f3f4167d05db",
   "metadata": {
    "language": "python",
    "name": "write_train_test"
   },
   "outputs": [],
   "source": [
    "train_df.write.save_as_table('HR_ANALYTICS.ML_MODELING.HR_EMPLOYEE_ATTRITION_TRAIN_DF', mode = 'overwrite')\n",
    "test_df.write.save_as_table('HR_ANALYTICS.ML_MODELING.HR_EMPLOYEE_ATTRITION_TEST_DF', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bc5eb7-3579-4e04-b67d-8833532a2a1c",
   "metadata": {
    "language": "python",
    "name": "read_train_test"
   },
   "outputs": [],
   "source": [
    "train_df = session.read.table(\"HR_ANALYTICS.ML_MODELING.HR_EMPLOYEE_ATTRITION_TRAIN_DF\")\n",
    "test_df = session.read.table(\"HR_ANALYTICS.ML_MODELING.HR_EMPLOYEE_ATTRITION_TEST_DF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03c9ddc-765c-4c19-955b-bc66e4736b49",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a3b603-1b2c-4832-8fba-3e72b89fcf1e",
   "metadata": {
    "language": "python",
    "name": "wh_up"
   },
   "outputs": [],
   "source": [
    "wh = str(session.get_current_warehouse()).strip('\"')\n",
    "print(f\"Current warehouse: {wh}\")\n",
    "print(session.sql(f\"SHOW WAREHOUSES LIKE '{wh}';\").collect())\n",
    "\n",
    "session.sql(f\"alter warehouse {session.get_current_warehouse()} set WAREHOUSE_SIZE = LARGE WAIT_FOR_COMPLETION = TRUE\").collect()\n",
    "\n",
    "print(session.sql(f\"SHOW WAREHOUSES LIKE '{wh}';\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f6aea2-871f-455b-b252-1f3cdc86e1b8",
   "metadata": {
    "language": "python",
    "name": "random_forest_train"
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# XGBOOST MODEL WITH GRID SEARCH\n",
    "# ========================================\n",
    "st.header(\"üöÄ XGBoost Classifier with Grid Search\")\n",
    "# Import required libraries\n",
    "from snowflake.ml.modeling.xgboost import XGBClassifier\n",
    "from snowflake.ml.modeling.model_selection import GridSearchCV\n",
    "from snowflake.ml.modeling.metrics import accuracy_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üöÄ Training XGBoost with Grid Search...\")\n",
    "\n",
    "# Define target column and feature columns\n",
    "target_column = ['ATTRITION']\n",
    "output_column = ['PRED_ATTRITION']\n",
    "exclude_columns = ['EMPLOYEE_NUMBER'] if 'EMPLOYEE_NUMBER' in train_df.columns else []\n",
    "\n",
    "# Get all feature columns (exclude target and passthrough columns)\n",
    "all_columns = train_df.columns\n",
    "model_feature_columns = [col for col in all_columns if col not in target_column and col not in exclude_columns]\n",
    "\n",
    "print(f\"üìä Using {len(model_feature_columns)} features for training\")\n",
    "print(f\"üéØ Target column: {target_column}\")\n",
    "\n",
    "\n",
    "# Create GridSearchCV\n",
    "model_pipeline = GridSearchCV(\n",
    "    estimator=XGBClassifier(),\n",
    "    param_grid={\n",
    "        'n_estimators': [50, 100],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': range(2,6,1)\n",
    "    },\n",
    "    n_jobs=-1,\n",
    "    input_cols=model_feature_columns,\n",
    "    passthrough_cols = exclude_columns,\n",
    "    label_cols=target_column,\n",
    "    output_cols=output_column,\n",
    ")\n",
    "\n",
    "print(\"üîß Fitting XGBoost model with Grid Search...\")\n",
    "\n",
    "# Fit the model with grid search\n",
    "fitted_model = model_pipeline.fit(train_df)\n",
    "\n",
    "print(\"‚úÖ XGBoost Grid Search training completed!\")\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "print(\"üîÆ Making predictions...\")\n",
    "xgb_gs_train = model_pipeline.predict(train_df)\n",
    "xgb_gs_predictions = model_pipeline.predict(test_df)\n",
    "\n",
    "st.success(\"‚úÖ **XGBoost with Grid Search trained successfully!**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fc3f8a-6f90-48f9-aecd-60d0c830f457",
   "metadata": {
    "language": "python",
    "name": "ACC_AUC"
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy scores\n",
    "train_accuracy = accuracy_score(df=xgb_gs_train, y_true_col_names=target_column, y_pred_col_names=output_column)\n",
    "test_accuracy = accuracy_score(df=xgb_gs_predictions, y_true_col_names=target_column, y_pred_col_names=output_column)\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "# ROC AUC scores\n",
    "train_auc = roc_auc_score(df=xgb_gs_train, y_true_col_names=target_column, y_score_col_names=output_column)\n",
    "test_auc = roc_auc_score(df=xgb_gs_predictions, y_true_col_names=target_column, y_score_col_names=output_column)\n",
    "print(f'Test AUC: {test_auc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2bf798-2e42-4c0f-9cc6-1e190976de6e",
   "metadata": {
    "language": "python",
    "name": "model_results"
   },
   "outputs": [],
   "source": [
    "# Grid Search Results Analysis and Feature Importance\n",
    "st.header(\"üìä Grid Search Analysis & Feature Importance\")\n",
    "st.markdown(\"---\")\n",
    "\n",
    "print(\"üìà Analyzing Grid Search Results...\")\n",
    "\n",
    "# Use the fitted model\n",
    "fitted_model = model_pipeline\n",
    "\n",
    "# Get grid search results\n",
    "gs_results = fitted_model.to_sklearn().cv_results_\n",
    "n_estimators_val = []\n",
    "learning_rate_val = []\n",
    "for param_dict in gs_results[\"params\"]:\n",
    "    n_estimators_val.append(param_dict[\"n_estimators\"])\n",
    "    learning_rate_val.append(param_dict[\"learning_rate\"])\n",
    "mape_val = gs_results[\"mean_test_score\"]\n",
    "\n",
    "gs_results_df = pd.DataFrame(data={\n",
    "    \"n_estimators\": n_estimators_val,\n",
    "    \"learning_rate\": learning_rate_val,\n",
    "    \"mape\": mape_val\n",
    "})\n",
    "\n",
    "st.subheader(\"üîç Grid Search Parameter Performance\")\n",
    "\n",
    "# Display grid search results summary\n",
    "col1, col2 = st.columns(2)\n",
    "\n",
    "with col1:\n",
    "    st.write(\"**üìã Grid Search Results Summary:**\")\n",
    "    st.dataframe(gs_results_df.sort_values('mape', ascending=False).head(10))\n",
    "\n",
    "with col2:\n",
    "    # Plot grid search results\n",
    "    sns.set_context(\"notebook\", font_scale=0.5)\n",
    "    fig = sns.relplot(data=gs_results_df, x=\"learning_rate\", y=\"mape\", hue=\"n_estimators\", kind=\"line\", height=3)\n",
    "    fig.set_xlabels('Learning Rate')\n",
    "    fig.set_ylabels('Mean Test Score (AUC)')\n",
    "    plt.title('Grid Search Results: Learning Rate vs Performance')\n",
    "    st.pyplot(fig)\n",
    "    plt.close()\n",
    "\n",
    "# Display best parameters and results\n",
    "st.subheader(\"üèÜ Best Model Results\")\n",
    "\n",
    "print(\"Results from Grid Search\")\n",
    "print(\"\\\\n The best estimator across ALL searched params:\\\\n\", fitted_model.to_sklearn().best_estimator_)\n",
    "print(\"\\\\n The best score across ALL searched params:\\\\n\", fitted_model.to_sklearn().best_score_)\n",
    "print(\"\\\\n The best parameters across ALL searched params:\\\\n\", fitted_model.to_sklearn().best_params_)\n",
    "\n",
    "# Display in Streamlit\n",
    "col1, col2, col3 = st.columns(3)\n",
    "\n",
    "with col1:\n",
    "    st.metric(\"Best Score (AUC)\", f\"{fitted_model.to_sklearn().best_score_:.4f}\")\n",
    "\n",
    "with col2:\n",
    "    best_params = fitted_model.to_sklearn().best_params_\n",
    "    st.write(\"**Best Parameters:**\")\n",
    "    for param, value in best_params.items():\n",
    "        st.write(f\"- {param}: {value}\")\n",
    "\n",
    "with col3:\n",
    "    st.write(\"**Model Performance:**\")\n",
    "    st.write(f\"- Training AUC: {train_auc:.4f}\")\n",
    "    st.write(f\"- Test AUC: {test_auc:.4f}\")\n",
    "    st.write(f\"- Training Accuracy: {train_accuracy:.4f}\")\n",
    "    st.write(f\"- Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Feature Importance Analysis\n",
    "st.subheader(\"üéØ Feature Importance Analysis\")\n",
    "\n",
    "# Get feature importance from the BEST estimator (this was the issue!)\n",
    "best_estimator = fitted_model.to_sklearn().best_estimator_\n",
    "feature_names = fitted_model.to_sklearn().feature_names_in_\n",
    "feature_importances = best_estimator.feature_importances_\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "feat_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'FeatImportance': feature_importances\n",
    "}).sort_values('FeatImportance', ascending=True)\n",
    "\n",
    "# Plot feature importance\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "feat_importance.plot.barh(x='Feature', y='FeatImportance', ax=ax, color='skyblue')\n",
    "ax.set_title('Feature Importance - XGBoost Model', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Feature Importance')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "st.pyplot(fig)\n",
    "plt.close()\n",
    "\n",
    "# Display top features\n",
    "st.subheader(\"üîù Top 10 Most Important Features\")\n",
    "top_features = feat_importance.tail(10).sort_values('FeatImportance', ascending=False)\n",
    "st.dataframe(top_features, use_container_width=True)\n",
    "\n",
    "print(\"‚úÖ Grid Search Analysis and Feature Importance completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1579edb-867b-4470-80e8-25cce215a088",
   "metadata": {
    "language": "python",
    "name": "wh_down"
   },
   "outputs": [],
   "source": [
    "wh = str(session.get_current_warehouse()).strip('\"')\n",
    "print(f\"Current warehouse: {wh}\")\n",
    "print(session.sql(f\"SHOW WAREHOUSES LIKE '{wh}';\").collect())\n",
    "\n",
    "session.sql(f\"alter warehouse {session.get_current_warehouse()} set WAREHOUSE_SIZE = XSMALL WAIT_FOR_COMPLETION = TRUE\").collect()\n",
    "\n",
    "print(session.sql(f\"SHOW WAREHOUSES LIKE '{wh}';\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5cc0f1-a395-41b5-bdb0-e01d6646b2ae",
   "metadata": {
    "language": "python",
    "name": "helper_fn"
   },
   "outputs": [],
   "source": [
    "# FUNCTION used to iterate the model version so we can automatically \n",
    "# create the next version number\n",
    "import ast\n",
    "import builtins  # Import the builtins module\n",
    "#from snowflake.snowpark import functions as F \n",
    "\n",
    "def get_next_version(reg, model_name) -> str:\n",
    "    \"\"\"\n",
    "    Returns the next version of a model based on the existing versions in the registry.\n",
    "\n",
    "    Args:\n",
    "        reg: The registry object that provides access to the models.\n",
    "        model_name: The name of the model.\n",
    "\n",
    "    Returns:\n",
    "        str: The next version of the model in the format \"V_\".\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the version list for the model is empty or if the version format is invalid.\n",
    "    \"\"\"\n",
    "    models = reg.show_models()\n",
    "    if models.empty:\n",
    "        return \"V_1\"\n",
    "    elif model_name not in models[\"name\"].to_list():\n",
    "        return \"V_1\"\n",
    "    max_version_number = builtins.max(  \n",
    "        [\n",
    "            int(version.split(\"_\")[-1])\n",
    "            for version in ast.literal_eval(\n",
    "                models.loc[models[\"name\"] == model_name, \"versions\"].values[0]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return f\"V_{max_version_number + 1}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cca85e-afcc-4837-8017-5d92d9164d55",
   "metadata": {
    "language": "python",
    "name": "reg_import"
   },
   "outputs": [],
   "source": [
    "# Let's now register the CV Classfier model into the model_registry\n",
    "Reg = Registry(\n",
    "    session=session,\n",
    "    database_name=session.get_current_database(),\n",
    "    schema_name='ML_MODELING',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a60e749-ebe3-4ef9-be03-b6369335f729",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "register_model"
   },
   "outputs": [],
   "source": [
    "model_name = 'EMPLOYEE_ATTRITION_XGBOOST'\n",
    "model_version = get_next_version(Reg, model_name)\n",
    "\n",
    "# Get model parameters for comment\n",
    "model_params = model_pipeline.to_sklearn().get_params()\n",
    "param_str = f\"n_estimators={model_params.get('n_estimators')}, learning_rate={model_params.get('learning_rate')}, max_depth={model_params.get('max_depth')}\"\n",
    "\n",
    "mv = Reg.log_model(fitted_model,\n",
    "    model_name=model_name,\n",
    "    version_name=model_version,\n",
    "    conda_dependencies=[\"snowflake-ml-python\"],\n",
    "    comment=f\"XGBoost model - Params: {param_str}\",\n",
    "    metrics={\"Test Acc\": test_accuracy, \"Test AUC\": test_auc, \"Train AUC\": train_auc, \"Train Acc\": train_accuracy}, # We can save our model metrics here\n",
    "    options= {\"relax_version\": False, \"enable_explainability\": True, 'case_sensitive': True},\n",
    "    \n",
    ")\n",
    "m = Reg.get_model(model_name)\n",
    "m.default = model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aef0d5d-9bf0-4425-9698-6c8a9914b6d5",
   "metadata": {
    "language": "python",
    "name": "check_model_versions"
   },
   "outputs": [],
   "source": [
    "Reg.get_model(model_name).show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c74b33-dc32-417d-b366-ba6dbbf8a6c3",
   "metadata": {
    "language": "python",
    "name": "select_latest_model"
   },
   "outputs": [],
   "source": [
    "prod_model = Reg.get_model(\"EMPLOYEE_ATTRITION_XGBOOST\").last() #or we can use .version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff772ca4-96d6-46b4-b16e-1cab7845b2cf",
   "metadata": {
    "language": "python",
    "name": "inference"
   },
   "outputs": [],
   "source": [
    "# prod_model.run(test_df, function_name = 'PREDICT_PROBA')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "harley.chen@snowflake.com",
   "authorId": "5126471379346",
   "authorName": "HCHEN",
   "lastEditTime": 1749919935451,
   "notebookId": "atklndfydjsshuedyo5q",
   "sessionId": "49cb54fe-0781-427a-8a06-b2f9ef10f689"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
